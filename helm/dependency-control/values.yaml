# -----------------------------------------------------------------------------
# Global Settings
# -----------------------------------------------------------------------------
global:
  imageRegistry: ""
  imagePullSecrets: []

  # ---------------------------------------------------------------------------
  # Internal TLS Configuration
  # ---------------------------------------------------------------------------
  tls:
    enabled: false
    # Certificate source: "certManager" or "custom"
    # - certManager: Uses cert-manager to generate certificates from internal CA
    # - custom: User provides their own certificates via Kubernetes secrets
    source: "certManager"
    # Cert-Manager Configuration (when source: certManager)
    certManager:
      # Use an existing Issuer/ClusterIssuer instead of creating internal CA
      # Leave empty to create a new self-signed CA
      existingIssuer:
        name: ""
        kind: "Issuer"  # or "ClusterIssuer"
      # Certificate settings
      certificate:
        duration: "8760h"      # 1 year
        renewBefore: "720h"    # 30 days before expiry
        privateKey:
          algorithm: "ECDSA"
          size: 256
    # Custom Certificates (when source: custom)
    custom:
      # CA certificate secret (must contain ca.crt)
      caSecretName: ""
      # Per-service TLS secrets (must contain tls.crt, tls.key, ca.crt)
      backend:
        secretName: ""
      frontend:
        secretName: ""
      database:
        secretName: ""
      cache:
        secretName: ""
    # CA mount path inside containers
    caMountPath: "/etc/ssl/certs/internal-ca"
# Environment identifier (used for validation)
environment: development

# -----------------------------------------------------------------------------
# Secrets Management
# -----------------------------------------------------------------------------
secrets:
  # Provider: kubernetes, external-secrets
  provider: kubernetes
  # Make secrets immutable (prevents modification after creation)
  # WARNING: When enabled, Helm upgrades cannot modify secrets - delete and recreate required
  # Recommended for production environments where secret rotation is handled externally
  immutable: false
  # External Secrets configuration (when provider: external-secrets)
  externalSecrets:
    enabled: false
    secretStore: vault-backend
    clusterSecretStore: false
    refreshInterval: 1h
    paths:
      backendSecretKey: "app/dependency-control/secret-key"
      dbPassword: "app/dependency-control/db-password"
      dbEncryptionKey: "app/dependency-control/db-encryption-key"
      dragonflyPassword: "app/dependency-control/dragonfly-password"

# -----------------------------------------------------------------------------
# Certificate Manager (Sub-Chart)
# -----------------------------------------------------------------------------
cert-manager:
  enabled: false
  installCRDs: false
  prometheus:
    enabled: true
  webhook:
    timeoutSeconds: 4

# -----------------------------------------------------------------------------
# Database Configuration
# -----------------------------------------------------------------------------
database:
  # Provider: mongodb (community operator) or percona (percona operator)
  type: mongodb
  # Authentication
  auth:
    username: dependencyuser
    password: ""  # Leave empty to auto-generate
    database: dependency_control
  # Cluster Configuration
  cluster:
    name: dependency-db
    replicas: 3
    image:
      repository: percona/percona-server-mongodb
      tag: 8.0.17-6
    resources:
      limits:
        cpu: "1"
        memory: "2Gi"
      requests:
        cpu: "500m"
        memory: "1Gi"
    persistence:
      enabled: true
      size: 20Gi
      storageClass: ""
  # Percona-Specific Configuration
  percona:
    # Percona Operator CRD version
    crVersion: "1.21.2"
    replicaSetName: rs0
    # Encryption at rest
    encryptionKey: ""  # Leave empty to auto-generate
    # TLS mode when global.tls.enabled: requireTLS, preferTLS, allowTLS
    tlsMode: requireTLS
    # Automatic version upgrades (disable for production!)
    upgradeOptions:
      apply: "disabled"  # disabled, never, recommended, latest
      schedule: "0 2 * * *"
      versionServiceEndpoint: "https://check.percona.com"
    backup:
      enabled: false
    pmm:
      enabled: false
    monitoring:
      enabled: false
      # MongoDB exporter sidecar resources
      exporter:
        resources:
          limits:
            cpu: 200m
            memory: 256Mi
          requests:
            cpu: 50m
            memory: 128Mi
  # MongoDB Community-Specific Configuration
  mongodb:
    version: "8.2.3"
    monitoring:
      enabled: false

# -----------------------------------------------------------------------------
# Service Account
# -----------------------------------------------------------------------------
serviceAccount:
  create: true
  annotations: {}
  name: ""
  automountServiceAccountToken: false

# -----------------------------------------------------------------------------
# Frontend Application
# -----------------------------------------------------------------------------
frontend:
  image:
    repository: ghcr.io/morzan1001/dependency-control-frontend
    pullPolicy: IfNotPresent
    tag: "latest"

  replicaCount: 2

  service:
    type: ClusterIP
    port: 80

  resources:
    limits:
      cpu: 200m
      memory: 256Mi
    requests:
      cpu: 50m
      memory: 128Mi

  securityContext:
    runAsUser: 101
    runAsGroup: 101
    fsGroup: 101
    runAsNonRoot: true
    readOnlyRootFilesystem: false  # Nginx needs /var/cache/nginx
    allowPrivilegeEscalation: false
    capabilities:
      drop: [ALL]

  env:
    apiUrl: "/api/v1"

  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 5
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # HPA behavior for controlled scaling (prevents flapping)
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 300  # Wait 5 min before scaling down
        policies:
          - type: Percent
            value: 50
            periodSeconds: 60
      scaleUp:
        stabilizationWindowSeconds: 60
        policies:
          - type: Percent
            value: 100
            periodSeconds: 30

  podDisruptionBudget:
    enabled: true
    minAvailable: 1

  probes:
    liveness:
      initialDelaySeconds: 30
      periodSeconds: 10
      failureThreshold: 3
      timeoutSeconds: 5
    readiness:
      initialDelaySeconds: 5
      periodSeconds: 5
      failureThreshold: 3
      timeoutSeconds: 5
    startup:
      initialDelaySeconds: 0
      periodSeconds: 5
      failureThreshold: 30
      timeoutSeconds: 5

# -----------------------------------------------------------------------------
# Backend Application
# -----------------------------------------------------------------------------
backend:
  image:
    repository: ghcr.io/morzan1001/dependency-control
    pullPolicy: IfNotPresent
    tag: "latest"

  replicaCount: 2

  service:
    type: ClusterIP
    port: 8000

  resources:
    limits:
      cpu: 1000m
      memory: 1Gi
    requests:
      cpu: 200m
      memory: 512Mi

  securityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
    runAsNonRoot: true
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities:
      drop: [ALL]

  # Graceful shutdown timeout (seconds)
  # Workers will finish current scan before shutdown, up to this limit
  # Should be > worker shutdown timeout (25s) to allow graceful completion
  terminationGracePeriodSeconds: 30

  env:
    workerCount: 2
    logLevel: "INFO"
    algorithm: "HS256"
    accessTokenExpireMinutes: 30
    refreshTokenExpireDays: 7
    frontendBaseUrl: ""
  # Secret key for JWT signing
  secrets:
    secretKey: ""  # Leave empty to auto-generate

  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # HPA behavior for controlled scaling (prevents flapping)
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 300  # Wait 5 min before scaling down
        policies:
          - type: Percent
            value: 50
            periodSeconds: 60
      scaleUp:
        stabilizationWindowSeconds: 60
        policies:
          - type: Percent
            value: 100
            periodSeconds: 30
          - type: Pods
            value: 4
            periodSeconds: 30
        selectPolicy: Max

  podDisruptionBudget:
    enabled: true
    minAvailable: 1

  probes:
    liveness:
      initialDelaySeconds: 30
      periodSeconds: 10
      failureThreshold: 3
      timeoutSeconds: 5
    readiness:
      initialDelaySeconds: 5
      periodSeconds: 5
      failureThreshold: 3
      timeoutSeconds: 5
    startup:
      initialDelaySeconds: 0
      periodSeconds: 5
      failureThreshold: 30
      timeoutSeconds: 5

  topologySpreadConstraints: []

# -----------------------------------------------------------------------------
# MongoDB Community Operator (Sub-Chart)
# -----------------------------------------------------------------------------
mongodb-operator:
  enabled: true
  community-operator-crds:
    enabled: true
  serviceAccount:
    create: true
    automountServiceAccountToken: true
  operator:
    watchNamespace: "dependency-control"
    podLabels:
      app.kubernetes.io/name: mongodb-kubernetes-operator

# -----------------------------------------------------------------------------
# Percona Operator (Sub-Chart)
# -----------------------------------------------------------------------------
psmdb-operator:
  enabled: false
  registerCRDs: true
  watchNamespace: "dependency-control"
  replicaCount: 1
  image:
    pullPolicy: IfNotPresent
  resources:
    limits:
      cpu: 200m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi

# -----------------------------------------------------------------------------
# DragonflyDB - Redis-compatible Cache (Sub-Chart)
# -----------------------------------------------------------------------------
dragonfly:
  enabled: true
  replicaCount: 2
  password: ""  # Leave empty to auto-generate

  resources:
    limits:
      cpu: 500m
      memory: 768Mi
    requests:
      cpu: 100m
      memory: 512Mi

  podSecurityContext:
    fsGroup: 1000
    runAsUser: 1000
    runAsGroup: 1000
    runAsNonRoot: true

  securityContext:
    allowPrivilegeEscalation: false
    readOnlyRootFilesystem: true
    runAsNonRoot: true
    runAsUser: 1000
    capabilities:
      drop: [ALL]

  extraArgs:
    - --maxmemory=512mb
    - --cache_mode=true
    - --proactor_threads=2
    # --requirepass needed to be set when not using autogenerate

  storage:
    enabled: false
    requests: 2Gi
    storageClassName: ""

  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: dragonfly
            topologyKey: kubernetes.io/hostname

  service:
    type: ClusterIP
    port: 6379

  serviceMonitor:
    enabled: false
    interval: "30s"
    scrapeTimeout: "10s"

  # PodDisruptionBudget for DragonflyDB
  podDisruptionBudget:
    enabled: true
    minAvailable: 1
    # maxUnavailable: 1  # Alternative to minAvailable
    # unhealthyPodEvictionPolicy: IfHealthyBudget  # Kubernetes 1.27+

# -----------------------------------------------------------------------------
# Traefik Ingress Controller (Sub-Chart)
# -----------------------------------------------------------------------------
traefik:
  enabled: true
  deployment:
    replicas: 2
    podDisruptionBudget:
      enabled: true
      minAvailable: 1
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values: [traefik]
            topologyKey: kubernetes.io/hostname
  ports:
    web:
      redirections:
        entryPoint:
          to: websecure
          scheme: https
          permanent: true
    websecure:
      tls:
        enabled: true
  additionalArguments:
    - "--entrypoints.websecure.http.tls=true"

# -----------------------------------------------------------------------------
# Ingress Configuration
# -----------------------------------------------------------------------------
ingress:
  enabled: true
  className: "traefik"
  hostname: "dependencycontrol.local"
  tls: true
  annotations: {}
  # Enable Traefik Middlewares when using external Traefik
  traefikMiddlewares:
    enabled: false

# -----------------------------------------------------------------------------
# Network Policies
# -----------------------------------------------------------------------------
networkPolicy:
  enabled: true
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app.kubernetes.io/name: traefik

# -----------------------------------------------------------------------------
# Monitoring & Observability
# -----------------------------------------------------------------------------
monitoring:
  enabled: false
  serviceMonitor:
    enabled: false
    interval: "30s"
    scrapeTimeout: "10s"
    tls:
      enabled: false
      insecureSkipVerify: false
    relabelings: []
  grafanaDashboard:
    enabled: false
    namespace: ""
    labels:
      grafana_dashboard: "1"
    annotations: {}
