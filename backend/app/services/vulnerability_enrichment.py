"""
Vulnerability Enrichment Service

Enriches vulnerability findings with additional data from external sources:
- EPSS (Exploit Prediction Scoring System) from FIRST.org
- CISA KEV (Known Exploited Vulnerabilities) catalog
- Additional metadata for prioritization

Data Sources:
- EPSS API: https://api.first.org/data/v1/epss
- CISA KEV: https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json

Caching:
- Uses Redis for distributed caching across all backend pods
- KEV catalog cached globally (updates daily)
- EPSS scores cached per-CVE (24 hour TTL)
- GHSA data cached per-ID (7 day TTL)
"""

import asyncio
import logging
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List, Optional, Tuple

import httpx

from app.core.cache import CacheKeys, CacheTTL, cache_service
from app.schemas.enrichment import EPSSData, GHSAData, KEVEntry, VulnerabilityEnrichment

logger = logging.getLogger(__name__)


class VulnerabilityEnrichmentService:
    """
    Service to enrich vulnerability data with EPSS scores and CISA KEV information.

    Uses Redis for distributed caching across all pods:
    - KEV catalog is cached globally for 24 hours (updates daily)
    - EPSS scores are cached per-CVE for 24 hours
    - GHSA resolutions are cached per-ID for 7 days (rarely change)

    This dramatically reduces API calls when running multiple backend replicas.
    """

    EPSS_API_URL = "https://api.first.org/data/v1/epss"
    KEV_CATALOG_URL = "https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json"
    GHSA_API_URL = "https://api.github.com/advisories"

    EPSS_BATCH_SIZE = 100  # Max CVEs per EPSS API request

    def __init__(self):
        # In-memory fallback caches (used only if Redis is unavailable)
        self._kev_cache_fallback: Dict[str, KEVEntry] = {}
        self._kev_cache_time_fallback: Optional[datetime] = None
        self._epss_cache_fallback: Dict[str, Tuple[EPSSData, datetime]] = {}
        self._ghsa_cache_fallback: Dict[str, Tuple[GHSAData, datetime]] = {}

        self._http_client: Optional[httpx.AsyncClient] = None
        self._max_retries = 3
        self._retry_delay = 1.0  # seconds
        # GitHub token for authenticated API access (higher rate limits)
        self._github_token: Optional[str] = None

    def set_github_token(self, token: Optional[str]) -> None:
        """Set the GitHub Personal Access Token for authenticated API requests."""
        self._github_token = token
        if token:
            logger.info("GitHub token configured - using authenticated API access")

    def _get_github_headers(self) -> Dict[str, str]:
        """Get headers for GitHub API requests, including auth if available."""
        headers = {
            "Accept": "application/vnd.github+json",
            "X-GitHub-Api-Version": "2022-11-28",
        }
        if self._github_token:
            headers["Authorization"] = f"Bearer {self._github_token}"
        return headers

    async def _get_client(self) -> httpx.AsyncClient:
        """Get or create HTTP client."""
        if self._http_client is None or self._http_client.is_closed:
            self._http_client = httpx.AsyncClient(timeout=30.0)
        return self._http_client

    async def close(self):
        """Close HTTP client."""
        if self._http_client and not self._http_client.is_closed:
            await self._http_client.aclose()

    async def _load_kev_catalog(self) -> Dict[str, KEVEntry]:
        """Load CISA KEV catalog, using Redis cache if available."""
        cache_key = CacheKeys.kev_catalog()

        # Try Redis cache first
        cached = await cache_service.get(cache_key)
        if cached:
            logger.debug("KEV catalog loaded from Redis cache")
            return {k: KEVEntry(**v) for k, v in cached.items()}

        # Fallback: check in-memory cache
        if self._kev_cache_time_fallback:
            cache_age = datetime.now(timezone.utc) - self._kev_cache_time_fallback
            if cache_age < timedelta(hours=24) and self._kev_cache_fallback:
                logger.debug("KEV catalog loaded from in-memory fallback cache")
                return self._kev_cache_fallback

        # Fetch fresh data from CISA
        try:
            client = await self._get_client()
            response = await client.get(self.KEV_CATALOG_URL)
            response.raise_for_status()

            data = response.json()
            vulnerabilities = data.get("vulnerabilities", [])

            kev_data = {}
            for vuln in vulnerabilities:
                cve = vuln.get("cveID", "")
                if cve:
                    kev_entry = KEVEntry(
                        cve=cve,
                        vendor_project=vuln.get("vendorProject", ""),
                        product=vuln.get("product", ""),
                        vulnerability_name=vuln.get("vulnerabilityName", ""),
                        date_added=vuln.get("dateAdded", ""),
                        short_description=vuln.get("shortDescription", ""),
                        required_action=vuln.get("requiredAction", ""),
                        due_date=vuln.get("dueDate", ""),
                        known_ransomware_use=vuln.get(
                            "knownRansomwareCampaignUse", ""
                        ).lower()
                        == "known",
                    )
                    kev_data[cve] = kev_entry

            # Cache in Redis (serialize to dict for JSON storage)
            kev_dict = {k: v.model_dump() for k, v in kev_data.items()}
            await cache_service.set(cache_key, kev_dict, CacheTTL.KEV_CATALOG)

            # Update fallback cache too
            self._kev_cache_fallback = kev_data
            self._kev_cache_time_fallback = datetime.now(timezone.utc)

            logger.info(
                f"Loaded {len(kev_data)} entries from CISA KEV catalog (cached in Redis)"
            )
            return kev_data

        except Exception as e:
            logger.warning(f"Failed to load CISA KEV catalog: {e}")
            # Return fallback cache if available
            return self._kev_cache_fallback

    # =========================================================================
    # GHSA Resolution Methods
    # =========================================================================

    async def _fetch_ghsa_advisory(self, ghsa_id: str) -> Optional[GHSAData]:
        """
        Fetch a single GitHub Security Advisory by its GHSA ID.

        Uses the GitHub Advisory Database API. If a GitHub token is configured,
        authenticated requests are made for higher rate limits.

        Checks Redis cache first before making API call.
        """
        # Check Redis cache first
        cache_key = CacheKeys.ghsa(ghsa_id)
        cached = await cache_service.get(cache_key)
        if cached:
            logger.debug(f"GHSA {ghsa_id} loaded from Redis cache")
            return GHSAData(**cached)

        try:
            client = await self._get_client()

            # GitHub Advisory API endpoint
            url = f"{self.GHSA_API_URL}/{ghsa_id}"

            headers = self._get_github_headers()

            response = await client.get(url, headers=headers, timeout=15.0)

            if response.status_code == 404:
                logger.debug(f"GHSA advisory not found: {ghsa_id}")
                # Cache negative result with shorter TTL
                empty_data = GHSAData(
                    ghsa_id=ghsa_id,
                    github_url=f"https://github.com/advisories/{ghsa_id}",
                )
                await cache_service.set(
                    cache_key, empty_data.model_dump(), CacheTTL.NEGATIVE_RESULT
                )
                return None

            if response.status_code == 403:
                # Rate limited
                logger.warning(f"GitHub API rate limited when fetching {ghsa_id}")
                return None

            response.raise_for_status()
            data = response.json()

            # Extract CVE from identifiers
            cve_id = None
            aliases = []
            for identifier in data.get("identifiers", []):
                id_type = identifier.get("type", "")
                id_value = identifier.get("value", "")
                if id_type == "CVE" and id_value:
                    cve_id = id_value
                elif id_value and id_value != ghsa_id:
                    aliases.append(id_value)

            # Also check aliases field
            for alias in data.get("aliases", []):
                if alias.startswith("CVE-") and not cve_id:
                    cve_id = alias
                elif alias not in aliases and alias != ghsa_id:
                    aliases.append(alias)

            ghsa_data = GHSAData(
                ghsa_id=ghsa_id,
                cve_id=cve_id,
                summary=data.get("summary"),
                severity=data.get("severity"),
                published_at=data.get("published_at"),
                updated_at=data.get("updated_at"),
                withdrawn_at=data.get("withdrawn_at"),
                github_url=data.get(
                    "html_url", f"https://github.com/advisories/{ghsa_id}"
                ),
                aliases=aliases,
            )

            # Cache in Redis
            await cache_service.set(
                cache_key, ghsa_data.model_dump(), CacheTTL.GHSA_DATA
            )
            logger.debug(f"GHSA {ghsa_id} fetched and cached in Redis")

            return ghsa_data

        except httpx.TimeoutException:
            logger.warning(f"Timeout fetching GHSA advisory: {ghsa_id}")
            return None
        except Exception as e:
            logger.warning(f"Failed to fetch GHSA advisory {ghsa_id}: {e}")
            return None

    async def resolve_ghsa_to_cve(self, ghsa_ids: List[str]) -> Dict[str, GHSAData]:
        """
        Resolve multiple GHSA IDs to CVEs and get advisory metadata.

        Uses Redis cache for previously resolved GHSAs.

        Args:
            ghsa_ids: List of GHSA IDs (e.g., ["GHSA-xxxx-xxxx-xxxx"])

        Returns:
            Dict mapping GHSA ID to GHSAData (includes CVE if available)
        """
        if not ghsa_ids:
            return {}

        results = {}
        missing_ghsas = []

        # Check Redis cache for each GHSA (batch get)
        cache_keys = [CacheKeys.ghsa(ghsa_id) for ghsa_id in ghsa_ids]
        cached_data = await cache_service.mget(cache_keys)

        for ghsa_id, cached in zip(ghsa_ids, cached_data.values()):
            if cached:
                results[ghsa_id] = GHSAData(**cached)
            else:
                missing_ghsas.append(ghsa_id)

        if missing_ghsas:
            logger.debug(f"Fetching {len(missing_ghsas)} GHSA advisories (cache miss)")

        # Fetch missing (one at a time to avoid rate limits)
        for ghsa_id in missing_ghsas:
            ghsa_data = await self._fetch_ghsa_advisory(ghsa_id)
            if ghsa_data:
                results[ghsa_id] = ghsa_data
            else:
                # Create empty data for failed lookups
                empty_data = GHSAData(
                    ghsa_id=ghsa_id,
                    github_url=f"https://github.com/advisories/{ghsa_id}",
                )
                results[ghsa_id] = empty_data

            # Small delay to avoid rate limiting
            await asyncio.sleep(0.1)

        logger.info(
            f"Resolved {len(results)} GHSA IDs ({len(ghsa_ids) - len(missing_ghsas)} from cache)"
        )
        return results

    def get_ghsa_url(self, ghsa_id: str) -> str:
        """Get the GitHub Advisory URL for a GHSA ID."""
        return f"https://github.com/advisories/{ghsa_id}"

    async def _fetch_epss_batch(self, cves: List[str]) -> Dict[str, EPSSData]:
        """Fetch EPSS scores for a batch of CVEs with retry logic."""
        if not cves:
            return {}

        last_error = None
        for attempt in range(self._max_retries):
            try:
                client = await self._get_client()
                # EPSS API accepts comma-separated CVE list
                cve_param = ",".join(cves)
                response = await client.get(
                    f"{self.EPSS_API_URL}?cve={cve_param}", timeout=30.0
                )
                response.raise_for_status()

                data = response.json()
                results = {}

                for entry in data.get("data", []):
                    cve = entry.get("cve", "")
                    if cve:
                        results[cve] = EPSSData(
                            cve=cve,
                            epss_score=float(entry.get("epss", 0)),
                            percentile=float(entry.get("percentile", 0))
                            * 100,  # Convert to percentage
                            date=entry.get("date", ""),
                        )

                # Log if some CVEs weren't found (not an error, just info)
                if len(results) < len(cves):
                    missing = set(cves) - set(results.keys())
                    if missing:
                        logger.debug(
                            f"EPSS: No data for {len(missing)} CVEs (may be too new or invalid)"
                        )

                return results

            except httpx.TimeoutException:
                last_error = "Timeout"
                logger.warning(
                    f"EPSS API timeout (attempt {attempt + 1}/{self._max_retries})"
                )
            except httpx.HTTPStatusError as e:
                last_error = f"HTTP {e.response.status_code}"
                if e.response.status_code == 429:  # Rate limited
                    wait_time = self._retry_delay * (2**attempt)  # Exponential backoff
                    logger.warning(f"EPSS API rate limited, waiting {wait_time}s")
                    await asyncio.sleep(wait_time)
                elif e.response.status_code >= 500:  # Server error
                    logger.warning(
                        f"EPSS API server error {e.response.status_code} (attempt {attempt + 1})"
                    )
                else:
                    # Client error (4xx except 429) - don't retry
                    logger.warning(f"EPSS API client error: {e}")
                    return {}
            except Exception as e:
                last_error = str(e)
                logger.warning(
                    f"Failed to fetch EPSS data (attempt {attempt + 1}): {e}"
                )

            if attempt < self._max_retries - 1:
                await asyncio.sleep(self._retry_delay)

        logger.error(
            f"EPSS API failed after {self._max_retries} attempts: {last_error}"
        )
        return {}

    async def _load_epss_scores(self, cves: List[str]) -> Dict[str, EPSSData]:
        """Load EPSS scores for given CVEs, using Redis cache where available."""
        result = {}
        missing_cves = []

        # Check Redis cache first (batch get)
        cache_keys = [CacheKeys.epss(cve) for cve in cves]
        cached_data = await cache_service.mget(cache_keys)

        for cve, cached in zip(cves, cached_data.values()):
            if cached:
                result[cve] = EPSSData(**cached)
            else:
                missing_cves.append(cve)

        # Fetch missing in batches from API
        if missing_cves:
            logger.debug(
                f"Fetching EPSS data for {len(missing_cves)} CVEs ({len(cves) - len(missing_cves)} from cache)"
            )

            for i in range(0, len(missing_cves), self.EPSS_BATCH_SIZE):
                batch = missing_cves[i : i + self.EPSS_BATCH_SIZE]
                batch_results = await self._fetch_epss_batch(batch)

                # Cache each result individually in Redis
                cache_mapping = {}
                for cve, data in batch_results.items():
                    cache_mapping[CacheKeys.epss(cve)] = data.model_dump()
                    result[cve] = data

                if cache_mapping:
                    await cache_service.mset(cache_mapping, CacheTTL.EPSS_SCORE)

                # Small delay between batches to be nice to the API
                if i + self.EPSS_BATCH_SIZE < len(missing_cves):
                    await asyncio.sleep(0.5)

        return result

    def _calculate_exploit_maturity(
        self, is_kev: bool, kev_ransomware: bool, epss_score: Optional[float]
    ) -> str:
        """
        Determine exploit maturity level.

        Levels:
        - weaponized: Active ransomware use
        - active: In CISA KEV (confirmed active exploitation)
        - high: EPSS > 0.1 (10% chance of exploitation)
        - medium: EPSS > 0.01 (1% chance)
        - low: EPSS <= 0.01
        - unknown: No data
        """
        if kev_ransomware:
            return "weaponized"
        if is_kev:
            return "active"
        if epss_score is not None:
            if epss_score > 0.1:
                return "high"
            if epss_score > 0.01:
                return "medium"
            return "low"
        return "unknown"

    def _calculate_risk_score(
        self,
        cvss_score: Optional[float],
        epss_score: Optional[float],
        is_kev: bool,
        kev_ransomware: bool,
        is_reachable: Optional[bool] = None,
        reachability_level: Optional[str] = None,
    ) -> float:
        """
        Calculate a combined risk score (0-100) considering:
        - CVSS score (severity/impact)
        - EPSS score (likelihood of exploitation in the wild)
        - KEV status (confirmed active exploitation)
        - Reachability (is the vulnerable code actually used?)

        Formula (revised for realistic prioritization):
        - Base: CVSS normalized to 0-40 (severity is important but not everything)
        - EPSS contribution: up to +25 points (exploitation probability is key)
        - KEV bonus: +20 points (actively exploited = urgent)
        - Ransomware bonus: +5 points (known ransomware = very urgent)
        - Reachability modifier: multiplier based on whether code is used
        """
        score = 0.0

        # CVSS contribution (0-40 points)
        # CVSS tells us the IMPACT if exploited, but not likelihood
        if cvss_score is not None:
            score += (cvss_score / 10.0) * 40
        else:
            # Default to medium if no CVSS
            score += 20

        # EPSS contribution (0-25 points)
        # EPSS tells us the PROBABILITY of exploitation in the next 30 days
        if epss_score is not None:
            # Non-linear scaling: high EPSS scores get disproportionately more weight
            if epss_score >= 0.1:  # Top 10% - very likely to be exploited
                epss_contribution = 20 + (min(epss_score, 1.0) * 5)
            elif epss_score >= 0.01:  # 1-10% - moderate likelihood
                epss_contribution = 10 + (epss_score * 100)  # 10-20 points
            else:  # < 1% - low likelihood
                epss_contribution = epss_score * 1000  # 0-10 points
            score += min(epss_contribution, 25)

        # KEV bonus - confirmed active exploitation is critical
        if is_kev:
            score += 20

        # Ransomware bonus - known ransomware campaigns need immediate attention
        if kev_ransomware:
            score += 5

        # Apply reachability modifier
        # If we know the code is unreachable, significantly reduce the score
        # If reachable or unknown, keep the score as-is or boost it slightly
        if is_reachable is not None or reachability_level is not None:
            if is_reachable is False or reachability_level == "unreachable":
                # Unreachable: reduce score by 60% (still keep some risk as analysis isn't perfect)
                score *= 0.4
            elif is_reachable is True or reachability_level in ("confirmed", "likely"):
                # Confirmed reachable: slight boost
                if reachability_level == "confirmed":
                    score *= 1.1  # 10% boost for confirmed reachability
                # "likely" gets no modifier - baseline score
        # If reachability is unknown/not analyzed, no modifier applied

        return min(score, 100.0)

    def calculate_adjusted_risk_score(
        self,
        base_risk_score: float,
        is_reachable: Optional[bool] = None,
        reachability_level: Optional[str] = None,
    ) -> float:
        """
        Calculate an adjusted risk score considering reachability.
        Use this when reachability is analyzed separately from EPSS/KEV.
        """
        if is_reachable is None and reachability_level is None:
            return base_risk_score

        if is_reachable is False or reachability_level == "unreachable":
            return base_risk_score * 0.4
        elif reachability_level == "confirmed":
            return min(base_risk_score * 1.1, 100.0)

        return base_risk_score

    async def enrich_cves(
        self,
        cves: List[str],
        cvss_scores: Optional[Dict[str, float]] = None,
    ) -> Dict[str, VulnerabilityEnrichment]:
        """
        Enrich a list of CVEs with EPSS and KEV data.

        Args:
            cves: List of CVE IDs to enrich
            cvss_scores: Optional dict of CVE -> CVSS score for risk calculation

        Returns:
            Dict mapping CVE ID to VulnerabilityEnrichment
        """
        if not cves:
            return {}

        # Deduplicate and filter valid CVE IDs
        unique_cves = list(set(cve for cve in cves if cve and cve.startswith("CVE-")))

        if not unique_cves:
            return {}

        cvss_scores = cvss_scores or {}

        # Load data sources in parallel
        kev_task = self._load_kev_catalog()
        epss_task = self._load_epss_scores(unique_cves)

        kev_catalog, epss_data = await asyncio.gather(kev_task, epss_task)

        # Build enrichment for each CVE
        results = {}
        for cve in unique_cves:
            kev_entry = kev_catalog.get(cve)
            epss_entry = epss_data.get(cve)
            cvss = cvss_scores.get(cve)

            is_kev = kev_entry is not None
            kev_ransomware = kev_entry.known_ransomware_use if kev_entry else False
            epss_score = epss_entry.epss_score if epss_entry else None

            enrichment = VulnerabilityEnrichment(
                cve=cve,
                epss_score=epss_score,
                epss_percentile=epss_entry.percentile if epss_entry else None,
                epss_date=epss_entry.date if epss_entry else None,
                is_kev=is_kev,
                kev_date_added=kev_entry.date_added if kev_entry else None,
                kev_due_date=kev_entry.due_date if kev_entry else None,
                kev_required_action=kev_entry.required_action if kev_entry else None,
                kev_ransomware_use=kev_ransomware,
                exploit_maturity=self._calculate_exploit_maturity(
                    is_kev, kev_ransomware, epss_score
                ),
                risk_score=self._calculate_risk_score(
                    cvss, epss_score, is_kev, kev_ransomware
                ),
            )

            results[cve] = enrichment

        logger.info(
            f"Enriched {len(results)} CVEs (KEV: {sum(1 for e in results.values() if e.is_kev)}, EPSS: {sum(1 for e in results.values() if e.epss_score is not None)})"
        )

        return results

    async def enrich_findings(
        self, findings: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Enrich a list of vulnerability findings with EPSS and KEV data.
        Modifies findings in-place and returns them.

        Args:
            findings: List of finding dicts with vulnerabilities in details

        Returns:
            Same list with enrichment data added to each finding
        """
        if not findings:
            return findings

        # Extract CVE IDs and CVSS scores from all findings
        # Map: CVE -> List of (finding, vuln_index) tuples to update
        cve_to_findings: Dict[str, List[Dict[str, Any]]] = {}
        cvss_scores: Dict[str, float] = {}

        for finding in findings:
            details = finding.get("details", {})
            if not isinstance(details, dict):
                continue

            # Check if finding ID itself is a CVE
            finding_id = finding.get("finding_id") or finding.get("id", "")
            if finding_id and finding_id.startswith("CVE-"):
                if finding_id not in cve_to_findings:
                    cve_to_findings[finding_id] = []
                cve_to_findings[finding_id].append(finding)
                if details.get("cvss_score"):
                    cvss_scores[finding_id] = details["cvss_score"]

            # Extract CVEs from vulnerabilities array (aggregated findings)
            vulns = details.get("vulnerabilities", [])
            for vuln in vulns:
                cve = vuln.get("id", "")
                if cve and (cve.startswith("CVE-") or cve.startswith("GHSA-")):
                    if cve not in cve_to_findings:
                        cve_to_findings[cve] = []
                    cve_to_findings[cve].append(finding)

                    # Extract CVSS score
                    if vuln.get("cvss_score") and cve not in cvss_scores:
                        cvss_scores[cve] = vuln["cvss_score"]

                # Also check aliases within the vulnerability
                for alias in vuln.get("aliases", []):
                    if alias.startswith("CVE-") and alias not in cve_to_findings:
                        cve_to_findings[alias] = []
                        cve_to_findings[alias].append(finding)

            # Also check aliases at finding level
            for alias in finding.get("aliases", []):
                if alias.startswith("CVE-") and alias not in cve_to_findings:
                    cve_to_findings[alias] = []
                    cve_to_findings[alias].append(finding)

        if not cve_to_findings:
            return findings

        # =====================================================================
        # GHSA Resolution: Resolve GHSA IDs to CVEs and add GitHub URLs
        # =====================================================================
        ghsa_ids = [vid for vid in cve_to_findings.keys() if vid.startswith("GHSA-")]
        ghsa_resolutions: Dict[str, GHSAData] = {}

        if ghsa_ids:
            logger.info(f"Resolving {len(ghsa_ids)} GHSA IDs to CVEs")
            ghsa_resolutions = await self.resolve_ghsa_to_cve(ghsa_ids)

            # Process resolved GHSAs
            for ghsa_id, ghsa_data in ghsa_resolutions.items():
                affected_findings = cve_to_findings.get(ghsa_id, [])

                for finding in affected_findings:
                    if "details" not in finding:
                        finding["details"] = {}

                    # Add GitHub Advisory URL to finding
                    finding["details"]["github_advisory_url"] = ghsa_data.advisory_url

                    # Update vulnerabilities array with GHSA data
                    vulns = finding["details"].get("vulnerabilities", [])
                    for vuln in vulns:
                        if vuln.get("id") == ghsa_id:
                            vuln["github_advisory_url"] = ghsa_data.advisory_url

                            # If we resolved a CVE, add it as an alias and use for EPSS/KEV
                            if ghsa_data.cve_id:
                                if "aliases" not in vuln:
                                    vuln["aliases"] = []
                                if ghsa_data.cve_id not in vuln["aliases"]:
                                    vuln["aliases"].append(ghsa_data.cve_id)
                                vuln["resolved_cve"] = ghsa_data.cve_id

                                # Add this CVE to our enrichment list
                                if ghsa_data.cve_id not in cve_to_findings:
                                    cve_to_findings[ghsa_data.cve_id] = []
                                if finding not in cve_to_findings[ghsa_data.cve_id]:
                                    cve_to_findings[ghsa_data.cve_id].append(finding)

                            # Add other aliases from GHSA
                            for alias in ghsa_data.aliases:
                                if alias not in vuln.get("aliases", []):
                                    if "aliases" not in vuln:
                                        vuln["aliases"] = []
                                    vuln["aliases"].append(alias)

                    # Also update finding-level aliases
                    if ghsa_data.cve_id:
                        if "aliases" not in finding:
                            finding["aliases"] = []
                        if ghsa_data.cve_id not in finding["aliases"]:
                            finding["aliases"].append(ghsa_data.cve_id)

        # Enrich CVEs (only CVE- prefixed, not GHSA-)
        cves_to_enrich = [
            cve for cve in cve_to_findings.keys() if cve.startswith("CVE-")
        ]
        enrichments = await self.enrich_cves(cves_to_enrich, cvss_scores)

        # Apply enrichment to findings and their vulnerabilities
        for cve, enrichment in enrichments.items():
            for finding in cve_to_findings.get(cve, []):
                # Add enrichment data to finding details
                if "details" not in finding:
                    finding["details"] = {}

                # Enrich individual vulnerabilities in the array
                vulns = finding["details"].get("vulnerabilities", [])
                for vuln in vulns:
                    vuln_id = vuln.get("id", "")
                    if vuln_id == cve or cve in vuln.get("aliases", []):
                        # Add enrichment to this specific vulnerability
                        if enrichment.epss_score is not None:
                            vuln["epss_score"] = enrichment.epss_score
                            vuln["epss_percentile"] = enrichment.epss_percentile
                        if enrichment.is_kev:
                            vuln["in_kev"] = True
                            vuln["kev_due_date"] = enrichment.kev_due_date
                            vuln["kev_ransomware_use"] = enrichment.kev_ransomware_use

                # Also add aggregated data to finding level for quick access
                # Use the highest EPSS score and any KEV status across all vulns
                max_epss = finding["details"].get("epss_score")
                if enrichment.epss_score is not None:
                    if max_epss is None or enrichment.epss_score > max_epss:
                        finding["details"]["epss_score"] = enrichment.epss_score
                        finding["details"][
                            "epss_percentile"
                        ] = enrichment.epss_percentile
                        finding["details"]["epss_date"] = enrichment.epss_date

                if enrichment.is_kev:
                    finding["details"]["in_kev"] = True
                    finding["details"]["kev_date_added"] = enrichment.kev_date_added
                    finding["details"]["kev_due_date"] = enrichment.kev_due_date
                    finding["details"][
                        "kev_required_action"
                    ] = enrichment.kev_required_action
                    finding["details"][
                        "kev_ransomware_use"
                    ] = enrichment.kev_ransomware_use

                if (
                    enrichment.exploit_maturity
                    and enrichment.exploit_maturity != "unknown"
                ):
                    current_maturity = finding["details"].get(
                        "exploit_maturity", "unknown"
                    )
                    # Keep the more severe maturity level
                    # Levels: unknown < low < medium < high < active < weaponized
                    maturity_order = {
                        "unknown": 0,
                        "low": 1,
                        "medium": 2,
                        "high": 3,
                        "poc": 4,  # Proof of concept
                        "active": 5,
                        "weaponized": 6,
                    }
                    if maturity_order.get(
                        enrichment.exploit_maturity, 0
                    ) > maturity_order.get(current_maturity, 0):
                        finding["details"][
                            "exploit_maturity"
                        ] = enrichment.exploit_maturity

                if enrichment.risk_score is not None:
                    current_risk = finding["details"].get("risk_score")
                    if current_risk is None or enrichment.risk_score > current_risk:
                        finding["details"]["risk_score"] = enrichment.risk_score

        return findings


# Singleton instance
vulnerability_enrichment_service = VulnerabilityEnrichmentService()


async def enrich_vulnerability_findings(
    findings: List[Dict[str, Any]],
    github_token: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """Convenience function to enrich findings.

    Args:
        findings: List of finding dicts to enrich
        github_token: Optional GitHub Personal Access Token for authenticated API access
    """
    # Set the GitHub token on the service if provided
    if github_token:
        vulnerability_enrichment_service.set_github_token(github_token)
    return await vulnerability_enrichment_service.enrich_findings(findings)


async def get_cve_enrichment(cves: List[str]) -> Dict[str, VulnerabilityEnrichment]:
    """Convenience function to get enrichment for CVE list."""
    return await vulnerability_enrichment_service.enrich_cves(cves)


async def resolve_ghsa_ids(ghsa_ids: List[str]) -> Dict[str, GHSAData]:
    """Convenience function to resolve GHSA IDs to CVEs."""
    return await vulnerability_enrichment_service.resolve_ghsa_to_cve(ghsa_ids)


def get_github_advisory_url(ghsa_id: str) -> str:
    """Get the GitHub Advisory URL for a GHSA ID."""
    return f"https://github.com/advisories/{ghsa_id}"
